\printoptioncategory{Barrier Parameter Update}
\printoption{adaptive\_mu\_globalization}%
{\ttfamily kkt-error, obj-constr-filter, never-monotone-mode}%
{obj-constr-filter}%
{Globalization strategy for the adaptive mu selection mode.\\
To achieve global convergence of the adaptive version, the algorithm has to switch to the monotone mode (Fiacco-McCormick approach) when convergence does not seem to appear.  This option sets the criterion used to decide when to do this switch. (Only used if option "mu\_strategy" is chosen as "adaptive".)}%
{\begin{list}{}{
\setlength{\parsep}{0em}
\setlength{\leftmargin}{5ex}
\setlength{\labelwidth}{2ex}
\setlength{\itemindent}{0ex}
\setlength{\topsep}{0pt}}
\item[\texttt{kkt-error}] nonmonotone decrease of kkt-error
\item[\texttt{obj-constr-filter}] 2-dim filter for objective and constraint violation
\item[\texttt{never-monotone-mode}] disables globalization
\end{list}
}

\printoption{adaptive\_mu\_kkt\_norm\_type}%
{\ttfamily 1-norm, 2-norm-squared, max-norm, 2-norm}%
{2-norm-squared}%
{Norm used for the KKT error in the adaptive mu globalization strategies.\\
When computing the KKT error for the globalization strategies, the norm to be used is specified with this option. Note, this options is also used in the QualityFunctionMuOracle.}%
{\begin{list}{}{
\setlength{\parsep}{0em}
\setlength{\leftmargin}{5ex}
\setlength{\labelwidth}{2ex}
\setlength{\itemindent}{0ex}
\setlength{\topsep}{0pt}}
\item[\texttt{1-norm}] use the 1-norm (abs sum)
\item[\texttt{2-norm-squared}] use the 2-norm squared (sum of squares)
\item[\texttt{max-norm}] use the infinity norm (max)
\item[\texttt{2-norm}] use 2-norm
\end{list}
}

\printoption{adaptive\_mu\_kkterror\_red\_fact}%
{$0<\textrm{real}<1$}%
{$0.9999$}%
{Sufficient decrease factor for "kkt-error" globalization strategy.\\
For the "kkt-error" based globalization strategy, the error must decrease by this factor to be deemed sufficient decrease.}%
{}

\printoption{adaptive\_mu\_kkterror\_red\_iters}%
{$0\leq\textrm{integer}$}%
{$4$}%
{Maximum number of iterations requiring sufficient progress.\\
For the "kkt-error" based globalization strategy, sufficient progress must be made for "adaptive\_mu\_kkterror\_red\_iters" iterations. If this number of iterations is exceeded, the globalization strategy switches to the monotone mode.}%
{}

\printoption{adaptive\_mu\_monotone\_init\_factor}%
{$0<\textrm{real}$}%
{$0.8$}%
{Determines the initial value of the barrier parameter when switching to the monotone mode.\\
When the globalization strategy for the adaptive barrier algorithm switches to the monotone mode and fixed\_mu\_oracle is chosen as "average\_compl", the barrier parameter is set to the current average complementarity times the value of "adaptive\_mu\_monotone\_init\_factor".}%
{}

\printoption{adaptive\_mu\_restore\_previous\_iterate}%
{\ttfamily no, yes}%
{no}%
{Indicates if the previous iterate should be restored if the monotone mode is entered.\\
When the globalization strategy for the adaptive barrier algorithm switches to the monotone mode, it can either start from the most recent iterate (no), or from the last iterate that was accepted (yes).}%
{\begin{list}{}{
\setlength{\parsep}{0em}
\setlength{\leftmargin}{5ex}
\setlength{\labelwidth}{2ex}
\setlength{\itemindent}{0ex}
\setlength{\topsep}{0pt}}
\item[\texttt{no}] don't restore accepted iterate
\item[\texttt{yes}] restore accepted iterate
\end{list}
}

\printoption{barrier\_tol\_factor}%
{$0<\textrm{real}$}%
{$10$}%
{Factor for mu in barrier stop test.\\
The convergence tolerance for each barrier problem in the monotone mode is the value of the barrier parameter times "barrier\_tol\_factor". This option is also used in the adaptive mu strategy during the monotone mode. (This is kappa\_epsilon in implementation paper).}%
{}

\printoption{filter\_margin\_fact}%
{$0<\textrm{real}<1$}%
{$10^{- 5}$}%
{Factor determining width of margin for obj-constr-filter adaptive globalization strategy.\\
When using the adaptive globalization strategy, "obj-constr-filter", sufficient progress for a filter entry is defined as follows: (new obj) $<$ (filter obj) - filter\_margin\_fact*(new constr-viol) OR (new constr-viol) $<$ (filter constr-viol) - filter\_margin\_fact*(new constr-viol).  For the description of the "kkt-error-filter" option see "filter\_max\_margin".}%
{}

\printoption{filter\_max\_margin}%
{$0<\textrm{real}$}%
{$1$}%
{Maximum width of margin in obj-constr-filter adaptive globalization strategy.}%
{}

\printoption{fixed\_mu\_oracle}%
{\ttfamily probing, loqo, quality-function, average\_compl}%
{average\_compl}%
{Oracle for the barrier parameter when switching to fixed mode.\\
Determines how the first value of the barrier parameter should be computed when switching to the "monotone mode" in the adaptive strategy. (Only considered if "adaptive" is selected for option "mu\_strategy".)}%
{\begin{list}{}{
\setlength{\parsep}{0em}
\setlength{\leftmargin}{5ex}
\setlength{\labelwidth}{2ex}
\setlength{\itemindent}{0ex}
\setlength{\topsep}{0pt}}
\item[\texttt{probing}] Mehrotra's probing heuristic
\item[\texttt{loqo}] LOQO's centrality rule
\item[\texttt{quality-function}] minimize a quality function
\item[\texttt{average\_compl}] base on current average complementarity
\end{list}
}

\printoption{mu\_allow\_fast\_monotone\_decrease}%
{\ttfamily no, yes}%
{yes}%
{Allow skipping of barrier problem if barrier test is already met.\\
If set to "no", the algorithm enforces at least one iteration per barrier problem, even if the barrier test is already met for the updated barrier parameter.}%
{\begin{list}{}{
\setlength{\parsep}{0em}
\setlength{\leftmargin}{5ex}
\setlength{\labelwidth}{2ex}
\setlength{\itemindent}{0ex}
\setlength{\topsep}{0pt}}
\item[\texttt{no}] Take at least one iteration per barrier problem
\item[\texttt{yes}] Allow fast decrease of mu if barrier test it met
\end{list}
}

\printoption{mu\_init}%
{$0<\textrm{real}$}%
{$0.1$}%
{Initial value for the barrier parameter.\\
This option determines the initial value for the barrier parameter (mu).  It is only relevant in the monotone, Fiacco-McCormick version of the algorithm. (i.e., if "mu\_strategy" is chosen as "monotone")}%
{}

\printoption{mu\_linear\_decrease\_factor}%
{$0<\textrm{real}<1$}%
{$0.2$}%
{Determines linear decrease rate of barrier parameter.\\
For the Fiacco-McCormick update procedure the new barrier parameter mu is obtained by taking the minimum of mu*"mu\_linear\_decrease\_factor" and mu\^"superlinear\_decrease\_power".  (This is kappa\_mu in implementation paper.) This option is also used in the adaptive mu strategy during the monotone mode.}%
{}

\printoption{mu\_max}%
{$0<\textrm{real}$}%
{$100000$}%
{Maximum value for barrier parameter.\\
This option specifies an upper bound on the barrier parameter in the adaptive mu selection mode.  If this option is set, it overwrites the effect of mu\_max\_fact. (Only used if option "mu\_strategy" is chosen as "adaptive".)}%
{}

\printoption{mu\_max\_fact}%
{$0<\textrm{real}$}%
{$1000$}%
{Factor for initialization of maximum value for barrier parameter.\\
This option determines the upper bound on the barrier parameter.  This upper bound is computed as the average complementarity at the initial point times the value of this option. (Only used if option "mu\_strategy" is chosen as "adaptive".)}%
{}

\printoption{mu\_min}%
{$0<\textrm{real}$}%
{$10^{-11}$}%
{Minimum value for barrier parameter.\\
This option specifies the lower bound on the barrier parameter in the adaptive mu selection mode. By default, it is set to the minimum of 1e-11 and min("tol","compl\_inf\_tol")/("barrier\_tol\_factor"+1), which should be a reasonable value. (Only used if option "mu\_strategy" is chosen as "adaptive".)}%
{}

\printoption{mu\_oracle}%
{\ttfamily probing, loqo, quality-function}%
{quality-function}%
{Oracle for a new barrier parameter in the adaptive strategy.\\
Determines how a new barrier parameter is computed in each "free-mode" iteration of the adaptive barrier parameter strategy. (Only considered if "adaptive" is selected for option "mu\_strategy").}%
{\begin{list}{}{
\setlength{\parsep}{0em}
\setlength{\leftmargin}{5ex}
\setlength{\labelwidth}{2ex}
\setlength{\itemindent}{0ex}
\setlength{\topsep}{0pt}}
\item[\texttt{probing}] Mehrotra's probing heuristic
\item[\texttt{loqo}] LOQO's centrality rule
\item[\texttt{quality-function}] minimize a quality function
\end{list}
}

\printoption{mu\_strategy}%
{\ttfamily monotone, adaptive}%
{adaptive}%
{Update strategy for barrier parameter.\\
Determines which barrier parameter update strategy is to be used.}%
{\begin{list}{}{
\setlength{\parsep}{0em}
\setlength{\leftmargin}{5ex}
\setlength{\labelwidth}{2ex}
\setlength{\itemindent}{0ex}
\setlength{\topsep}{0pt}}
\item[\texttt{monotone}] use the monotone (Fiacco-McCormick) strategy
\item[\texttt{adaptive}] use the adaptive update strategy
\end{list}
}

\printoption{mu\_superlinear\_decrease\_power}%
{$1<\textrm{real}<2$}%
{$1.5$}%
{Determines superlinear decrease rate of barrier parameter.\\
For the Fiacco-McCormick update procedure the new barrier parameter mu is obtained by taking the minimum of mu*"mu\_linear\_decrease\_factor" and mu\^"superlinear\_decrease\_power".  (This is theta\_mu in implementation paper.) This option is also used in the adaptive mu strategy during the monotone mode.}%
{}

\printoption{quality\_function\_balancing\_term}%
{\ttfamily none, cubic}%
{none}%
{The balancing term included in the quality function for centrality.\\
This determines whether a term is added to the quality function that penalizes situations where the complementarity is much smaller than dual and primal infeasibilities. (Only used if option "mu\_oracle" is set to "quality-function".)}%
{\begin{list}{}{
\setlength{\parsep}{0em}
\setlength{\leftmargin}{5ex}
\setlength{\labelwidth}{2ex}
\setlength{\itemindent}{0ex}
\setlength{\topsep}{0pt}}
\item[\texttt{none}] no balancing term is added
\item[\texttt{cubic}] Max(0,Max(dual\_inf,primal\_inf)-compl)\^3
\end{list}
}

\printoption{quality\_function\_centrality}%
{\ttfamily none, log, reciprocal, cubed-reciprocal}%
{none}%
{The penalty term for centrality that is included in quality function.\\
This determines whether a term is added to the quality function to penalize deviation from centrality with respect to complementarity.  The complementarity measure here is the xi in the Loqo update rule. (Only used if option "mu\_oracle" is set to "quality-function".)}%
{\begin{list}{}{
\setlength{\parsep}{0em}
\setlength{\leftmargin}{5ex}
\setlength{\labelwidth}{2ex}
\setlength{\itemindent}{0ex}
\setlength{\topsep}{0pt}}
\item[\texttt{none}] no penalty term is added
\item[\texttt{log}] complementarity * the log of the centrality measure
\item[\texttt{reciprocal}] complementarity * the reciprocal of the centrality measure
\item[\texttt{cubed-reciprocal}] complementarity * the reciprocal of the centrality measure cubed
\end{list}
}

\printoption{quality\_function\_max\_section\_steps}%
{$0\leq\textrm{integer}$}%
{$8$}%
{Maximum number of search steps during direct search procedure determining the optimal centering parameter.\\
The golden section search is performed for the quality function based mu oracle. (Only used if option "mu\_oracle" is set to "quality-function".)}%
{}

\printoption{quality\_function\_norm\_type}%
{\ttfamily 1-norm, 2-norm-squared, max-norm, 2-norm}%
{2-norm-squared}%
{Norm used for components of the quality function.\\
(Only used if option "mu\_oracle" is set to "quality-function".)}%
{\begin{list}{}{
\setlength{\parsep}{0em}
\setlength{\leftmargin}{5ex}
\setlength{\labelwidth}{2ex}
\setlength{\itemindent}{0ex}
\setlength{\topsep}{0pt}}
\item[\texttt{1-norm}] use the 1-norm (abs sum)
\item[\texttt{2-norm-squared}] use the 2-norm squared (sum of squares)
\item[\texttt{max-norm}] use the infinity norm (max)
\item[\texttt{2-norm}] use 2-norm
\end{list}
}

\printoption{quality\_function\_section\_qf\_tol}%
{$0\leq\textrm{real}<1$}%
{$0$}%
{Tolerance for the golden section search procedure determining the optimal centering parameter (in the function value space).\\
The golden section search is performed for the quality function based mu oracle. (Only used if option "mu\_oracle" is set to "quality-function".)}%
{}

\printoption{quality\_function\_section\_sigma\_tol}%
{$0\leq\textrm{real}<1$}%
{$0.01$}%
{Tolerance for the section search procedure determining the optimal centering parameter (in sigma space).\\
The golden section search is performed for the quality function based mu oracle. (Only used if option "mu\_oracle" is set to "quality-function".)}%
{}

\printoption{sigma\_max}%
{$0<\textrm{real}$}%
{$100$}%
{Maximum value of the centering parameter.\\
This is the upper bound for the centering parameter chosen by the quality function based barrier parameter update. (Only used if option "mu\_oracle" is set to "quality-function".)}%
{}

\printoption{sigma\_min}%
{$0\leq\textrm{real}$}%
{$10^{- 6}$}%
{Minimum value of the centering parameter.\\
This is the lower bound for the centering parameter chosen by the quality function based barrier parameter update. (Only used if option "mu\_oracle" is set to "quality-function".)}%
{}

\printoption{tau\_min}%
{$0<\textrm{real}<1$}%
{$0.99$}%
{Lower bound on fraction-to-the-boundary parameter tau.\\
(This is tau\_min in the implementation paper.)  This option is also used in the adaptive mu strategy during the monotone mode.}%
{}

\printoptioncategory{Convergence}
\printoption{acceptable\_compl\_inf\_tol}%
{$0<\textrm{real}$}%
{$0.01$}%
{"Acceptance" threshold for the complementarity conditions.\\
Absolute tolerance on the complementarity. "Acceptable" termination requires that the max-norm of the (unscaled) complementarity is less than this threshold; see also acceptable\_tol.}%
{}

\printoption{acceptable\_constr\_viol\_tol}%
{$0<\textrm{real}$}%
{$0.01$}%
{"Acceptance" threshold for the constraint violation.\\
Absolute tolerance on the constraint violation. "Acceptable" termination requires that the max-norm of the (unscaled) constraint violation is less than this threshold; see also acceptable\_tol.}%
{}

\printoption{acceptable\_dual\_inf\_tol}%
{$0<\textrm{real}$}%
{$10^{ 10}$}%
{"Acceptance" threshold for the dual infeasibility.\\
Absolute tolerance on the dual infeasibility. "Acceptable" termination requires that the (max-norm of the unscaled) dual infeasibility is less than this threshold; see also acceptable\_tol.}%
{}

\printoption{acceptable\_iter}%
{$0\leq\textrm{integer}$}%
{$15$}%
{Number of "acceptable" iterates before triggering termination.\\
If the algorithm encounters this many successive "acceptable" iterates (see "acceptable\_tol"), it terminates, assuming that the problem has been solved to best possible accuracy given round-off.  If it is set to zero, this heuristic is disabled.}%
{}

\printoption{acceptable\_obj\_change\_tol}%
{$0\leq\textrm{real}$}%
{$10^{ 20}$}%
{"Acceptance" stopping criterion based on objective function change.\\
If the relative change of the objective function (scaled by Max(1,|f(x)|)) is less than this value, this part of the acceptable tolerance termination is satisfied; see also acceptable\_tol.  This is useful for the quasi-Newton option, which has trouble to bring down the dual infeasibility.}%
{}

\printoption{acceptable\_tol}%
{$0<\textrm{real}$}%
{$10^{- 6}$}%
{"Acceptable" convergence tolerance (relative).\\
Determines which (scaled) overall optimality error is considered to be "acceptable." There are two levels of termination criteria.  If the usual "desired" tolerances (see tol, dual\_inf\_tol etc) are satisfied at an iteration, the algorithm immediately terminates with a success message.  On the other hand, if the algorithm encounters "acceptable\_iter" many iterations in a row that are considered "acceptable", it will terminate before the desired convergence tolerance is met. This is useful in cases where the algorithm might not be able to achieve the "desired" level of accuracy.}%
{}

\printoption{compl\_inf\_tol}%
{$0<\textrm{real}$}%
{$0.0001$}%
{Desired threshold for the complementarity conditions.\\
Absolute tolerance on the complementarity. Successful termination requires that the max-norm of the (unscaled) complementarity is less than this threshold.}%
{}

\printoption{constr\_viol\_tol}%
{$0<\textrm{real}$}%
{$0.0001$}%
{Desired threshold for the constraint violation.\\
Absolute tolerance on the constraint violation. Successful termination requires that the max-norm of the (unscaled) constraint violation is less than this threshold.}%
{}

\printoption{diverging\_iterates\_tol}%
{$0<\textrm{real}$}%
{$10^{ 20}$}%
{Threshold for maximal value of primal iterates.\\
If any component of the primal iterates exceeded this value (in absolute terms), the optimization is aborted with the exit message that the iterates seem to be diverging.}%
{}

\printoption{dual\_inf\_tol}%
{$0<\textrm{real}$}%
{$1$}%
{Desired threshold for the dual infeasibility.\\
Absolute tolerance on the dual infeasibility. Successful termination requires that the max-norm of the (unscaled) dual infeasibility is less than this threshold.}%
{}

\printoption{max\_cpu\_time}%
{$0<\textrm{real}$}%
{$1000$}%
{Maximum number of CPU seconds.\\
A limit on CPU seconds that Ipopt can use to solve one problem.  If during the convergence check this limit is exceeded, Ipopt will terminate with a corresponding error message.}%
{}

\printoption{max\_iter}%
{$0\leq\textrm{integer}$}%
{$\infty$}%
{Maximum number of iterations.\\
The algorithm terminates with an error message if the number of iterations exceeded this number.}%
{}

\printoption{mu\_target}%
{$0\leq\textrm{real}$}%
{$0$}%
{Desired value of complementarity.\\
Usually, the barrier parameter is driven to zero and the termination test for complementarity is measured with respect to zero complementarity.  However, in some cases it might be desired to have Ipopt solve barrier problem for strictly positive value of the barrier parameter.  In this case, the value of "mu\_target" specifies the final value of the barrier parameter, and the termination tests are then defined with respect to the barrier problem for this value of the barrier parameter.}%
{}

\printoption{s\_max}%
{$0<\textrm{real}$}%
{$100$}%
{Scaling threshold for the NLP error.\\
(See paragraph after Eqn. (6) in the implementation paper.)}%
{}

\printoption{tol}%
{$0<\textrm{real}$}%
{$10^{- 8}$}%
{Desired convergence tolerance (relative).\\
Determines the convergence tolerance for the algorithm.  The algorithm terminates successfully, if the (scaled) NLP error becomes smaller than this value, and if the (absolute) criteria according to "dual\_inf\_tol", "primal\_inf\_tol", and "cmpl\_inf\_tol" are met.  (This is epsilon\_tol in Eqn. (6) in implementation paper).  See also "acceptable\_tol" as a second termination criterion.  Note, some other algorithmic features also use this quantity to determine thresholds etc.}%
{}

\printoptioncategory{Hessian Approximation}
\printoption{hessian\_approximation}%
{\ttfamily exact, limited-memory}%
{exact}%
{Indicates what Hessian information is to be used.\\
This determines which kind of information for the Hessian of the Lagrangian function is used by the algorithm.}%
{\begin{list}{}{
\setlength{\parsep}{0em}
\setlength{\leftmargin}{5ex}
\setlength{\labelwidth}{2ex}
\setlength{\itemindent}{0ex}
\setlength{\topsep}{0pt}}
\item[\texttt{exact}] Use second derivatives provided by the NLP.
\item[\texttt{limited-memory}] Perform a limited-memory quasi-Newton approximation
\end{list}
}

\printoption{hessian\_approximation\_space}%
{\ttfamily nonlinear-variables, all-variables}%
{nonlinear-variables}%
{Indicates in which subspace the Hessian information is to be approximated.}%
{\begin{list}{}{
\setlength{\parsep}{0em}
\setlength{\leftmargin}{5ex}
\setlength{\labelwidth}{2ex}
\setlength{\itemindent}{0ex}
\setlength{\topsep}{0pt}}
\item[\texttt{nonlinear-variables}] only in space of nonlinear variables.
\item[\texttt{all-variables}] in space of all variables (without slacks)
\end{list}
}

\printoption{limited\_memory\_aug\_solver}%
{\ttfamily sherman-morrison, extended}%
{sherman-morrison}%
{Strategy for solving the augmented system for low-rank Hessian.}%
{\begin{list}{}{
\setlength{\parsep}{0em}
\setlength{\leftmargin}{5ex}
\setlength{\labelwidth}{2ex}
\setlength{\itemindent}{0ex}
\setlength{\topsep}{0pt}}
\item[\texttt{sherman-morrison}] use Sherman-Morrison formula
\item[\texttt{extended}] use an extended augmented system
\end{list}
}

\printoption{limited\_memory\_init\_val}%
{$0<\textrm{real}$}%
{$1$}%
{Value for B0 in low-rank update.\\
The starting matrix in the low rank update, B0, is chosen to be this multiple of the identity in the first iteration (when no updates have been performed yet), and is constantly chosen as this value, if "limited\_memory\_initialization" is "constant".}%
{}

\printoption{limited\_memory\_init\_val\_max}%
{$0<\textrm{real}$}%
{$10^{  8}$}%
{Upper bound on value for B0 in low-rank update.\\
The starting matrix in the low rank update, B0, is chosen to be this multiple of the identity in the first iteration (when no updates have been performed yet), and is constantly chosen as this value, if "limited\_memory\_initialization" is "constant".}%
{}

\printoption{limited\_memory\_init\_val\_min}%
{$0<\textrm{real}$}%
{$10^{- 8}$}%
{Lower bound on value for B0 in low-rank update.\\
The starting matrix in the low rank update, B0, is chosen to be this multiple of the identity in the first iteration (when no updates have been performed yet), and is constantly chosen as this value, if "limited\_memory\_initialization" is "constant".}%
{}

\printoption{limited\_memory\_initialization}%
{\ttfamily scalar1, scalar2, scalar3, scalar4, constant}%
{scalar1}%
{Initialization strategy for the limited memory quasi-Newton approximation.\\
Determines how the diagonal Matrix $B_0$ as the first term in the limited memory approximation should be computed.}%
{\begin{list}{}{
\setlength{\parsep}{0em}
\setlength{\leftmargin}{5ex}
\setlength{\labelwidth}{2ex}
\setlength{\itemindent}{0ex}
\setlength{\topsep}{0pt}}
\item[\texttt{scalar1}] sigma = $s^Ty/s^Ts$
\item[\texttt{scalar2}] sigma = $y^Ty/s^Ty$
\item[\texttt{scalar3}] arithmetic average of scalar1 and scalar2
\item[\texttt{scalar4}] geometric average of scalar1 and scalar2
\item[\texttt{constant}] sigma = limited\_memory\_init\_val
\end{list}
}

\printoption{limited\_memory\_max\_history}%
{$0\leq\textrm{integer}$}%
{$6$}%
{Maximum size of the history for the limited quasi-Newton Hessian approximation.\\
This option determines the number of most recent iterations that are taken into account for the limited-memory quasi-Newton approximation.}%
{}

\printoption{limited\_memory\_max\_skipping}%
{$1\leq\textrm{integer}$}%
{$2$}%
{Threshold for successive iterations where update is skipped.\\
If the update is skipped more than this number of successive iterations, we quasi-Newton approximation is reset.}%
{}

\printoption{limited\_memory\_update\_type}%
{\ttfamily bfgs, sr1}%
{bfgs}%
{Quasi-Newton update formula for the limited memory approximation.\\
Determines which update formula is to be used for the limited-memory quasi-Newton approximation.}%
{\begin{list}{}{
\setlength{\parsep}{0em}
\setlength{\leftmargin}{5ex}
\setlength{\labelwidth}{2ex}
\setlength{\itemindent}{0ex}
\setlength{\topsep}{0pt}}
\item[\texttt{bfgs}] BFGS update (with skipping)
\item[\texttt{sr1}] SR1 (not working well)
\end{list}
}

\printoptioncategory{Initialization}
\printoption{bound\_frac}%
{$0<\textrm{real}\leq0.5$}%
{$0.01$}%
{Desired minimum relative distance from the initial point to bound.\\
Determines how much the initial point might have to be modified in order to be sufficiently inside the bounds (together with "bound\_push").  (This is kappa\_2 in Section 3.6 of implementation paper.)}%
{}

\printoption{bound\_mult\_init\_method}%
{\ttfamily constant, mu-based}%
{constant}%
{Initialization method for bound multipliers\\
This option defines how the iterates for the bound multipliers are initialized.  If "constant" is chosen, then all bound multipliers are initialized to the value of "bound\_mult\_init\_val".  If "mu-based" is chosen, the each value is initialized to the the value of "mu\_init" divided by the corresponding slack variable.  This latter option might be useful if the starting point is close to the optimal solution.}%
{\begin{list}{}{
\setlength{\parsep}{0em}
\setlength{\leftmargin}{5ex}
\setlength{\labelwidth}{2ex}
\setlength{\itemindent}{0ex}
\setlength{\topsep}{0pt}}
\item[\texttt{constant}] set all bound multipliers to the value of bound\_mult\_init\_val
\item[\texttt{mu-based}] initialize to mu\_init/x\_slack
\end{list}
}

\printoption{bound\_mult\_init\_val}%
{$0<\textrm{real}$}%
{$1$}%
{Initial value for the bound multipliers.\\
All dual variables corresponding to bound constraints are initialized to this value.}%
{}

\printoption{bound\_push}%
{$0<\textrm{real}$}%
{$0.01$}%
{Desired minimum absolute distance from the initial point to bound.\\
Determines how much the initial point might have to be modified in order to be sufficiently inside the bounds (together with "bound\_frac").  (This is kappa\_1 in Section 3.6 of implementation paper.)}%
{}

\printoption{constr\_mult\_init\_max}%
{$0\leq\textrm{real}$}%
{$1000$}%
{Maximum allowed least-square guess of constraint multipliers.\\
Determines how large the initial least-square guesses of the constraint multipliers are allowed to be (in max-norm). If the guess is larger than this value, it is discarded and all constraint multipliers are set to zero.  This options is also used when initializing the restoration phase. By default, "resto.constr\_mult\_init\_max" (the one used in RestoIterateInitializer) is set to zero.}%
{}

\printoption{least\_square\_init\_duals}%
{\ttfamily no, yes}%
{no}%
{Least square initialization of all dual variables\\
If set to yes, Ipopt tries to compute least-square multipliers (considering ALL dual variables).  If successful, the bound multipliers are possibly corrected to be at least bound\_mult\_init\_val. This might be useful if the user doesn't know anything about the starting point, or for solving an LP or QP.  This overwrites option "bound\_mult\_init\_method".}%
{\begin{list}{}{
\setlength{\parsep}{0em}
\setlength{\leftmargin}{5ex}
\setlength{\labelwidth}{2ex}
\setlength{\itemindent}{0ex}
\setlength{\topsep}{0pt}}
\item[\texttt{no}] use bound\_mult\_init\_val and least-square equality constraint multipliers
\item[\texttt{yes}] overwrite user-provided point with least-square estimates
\end{list}
}

\printoption{least\_square\_init\_primal}%
{\ttfamily no, yes}%
{no}%
{Least square initialization of the primal variables\\
If set to yes, Ipopt ignores the user provided point and solves a least square problem for the primal variables (x and s), to fit the linearized equality and inequality constraints.  This might be useful if the user doesn't know anything about the starting point, or for solving an LP or QP.}%
{\begin{list}{}{
\setlength{\parsep}{0em}
\setlength{\leftmargin}{5ex}
\setlength{\labelwidth}{2ex}
\setlength{\itemindent}{0ex}
\setlength{\topsep}{0pt}}
\item[\texttt{no}] take user-provided point
\item[\texttt{yes}] overwrite user-provided point with least-square estimates
\end{list}
}

\printoption{slack\_bound\_frac}%
{$0<\textrm{real}\leq0.5$}%
{$0.01$}%
{Desired minimum relative distance from the initial slack to bound.\\
Determines how much the initial slack variables might have to be modified in order to be sufficiently inside the inequality bounds (together with "slack\_bound\_push").  (This is kappa\_2 in Section 3.6 of implementation paper.)}%
{}

\printoption{slack\_bound\_push}%
{$0<\textrm{real}$}%
{$0.01$}%
{Desired minimum absolute distance from the initial slack to bound.\\
Determines how much the initial slack variables might have to be modified in order to be sufficiently inside the inequality bounds (together with "slack\_bound\_frac").  (This is kappa\_1 in Section 3.6 of implementation paper.)}%
{}

\printoptioncategory{Line Search}
\printoption{accept\_after\_max\_steps}%
{$-1\leq\textrm{integer}$}%
{$-1$}%
{Accept a trial point after maximal this number of steps.\\
Even if it does not satisfy line search conditions.}%
{}

\printoption{accept\_every\_trial\_step}%
{\ttfamily no, yes}%
{no}%
{Always accept the first trial step.\\
Setting this option to "yes" essentially disables the line search and makes the algorithm take aggressive steps, without global convergence guarantees.}%
{\begin{list}{}{
\setlength{\parsep}{0em}
\setlength{\leftmargin}{5ex}
\setlength{\labelwidth}{2ex}
\setlength{\itemindent}{0ex}
\setlength{\topsep}{0pt}}
\item[\texttt{no}] don't arbitrarily accept the full step
\item[\texttt{yes}] always accept the full step
\end{list}
}

\printoption{alpha\_for\_y}%
{\ttfamily primal, bound-mult, min, max, full, min-dual-infeas, safer-min-dual-infeas, primal-and-full, dual-and-full, acceptor}%
{primal}%
{Method to determine the step size for constraint multipliers.\\
This option determines how the step size (alpha\_y) will be calculated when updating the constraint multipliers.}%
{\begin{list}{}{
\setlength{\parsep}{0em}
\setlength{\leftmargin}{5ex}
\setlength{\labelwidth}{2ex}
\setlength{\itemindent}{0ex}
\setlength{\topsep}{0pt}}
\item[\texttt{primal}] use primal step size
\item[\texttt{bound-mult}] use step size for the bound multipliers (good for LPs)
\item[\texttt{min}] use the min of primal and bound multipliers
\item[\texttt{max}] use the max of primal and bound multipliers
\item[\texttt{full}] take a full step of size one
\item[\texttt{min-dual-infeas}] choose step size minimizing new dual infeasibility
\item[\texttt{safer-min-dual-infeas}] like "min\_dual\_infeas", but safeguarded by "min" and "max"
\item[\texttt{primal-and-full}] use the primal step size, and full step if delta\_x $<$= alpha\_for\_y\_tol
\item[\texttt{dual-and-full}] use the dual step size, and full step if delta\_x $<$= alpha\_for\_y\_tol
\item[\texttt{acceptor}] Call LSAcceptor to get step size for y
\end{list}
}

\printoption{alpha\_for\_y\_tol}%
{$0\leq\textrm{real}$}%
{$10$}%
{Tolerance for switching to full equality multiplier steps.\\
This is only relevant if "alpha\_for\_y" is chosen "primal-and-full" or "dual-and-full".  The step size for the equality constraint multipliers is taken to be one if the max-norm of the primal step is less than this tolerance.}%
{}

\printoption{alpha\_min\_frac}%
{$0<\textrm{real}<1$}%
{$0.05$}%
{Safety factor for the minimal step size (before switching to restoration phase).\\
(This is gamma\_alpha in Eqn. (20) in the implementation paper.)}%
{}

\printoption{alpha\_red\_factor}%
{$0<\textrm{real}<1$}%
{$0.5$}%
{Fractional reduction of the trial step size in the backtracking line search.\\
At every step of the backtracking line search, the trial step size is reduced by this factor.}%
{}

\printoption{constraint\_violation\_norm\_type}%
{\ttfamily 1-norm, 2-norm, max-norm}%
{1-norm}%
{Norm to be used for the constraint violation in the line search.\\
Determines which norm should be used when the algorithm computes the constraint violation in the line search.}%
{\begin{list}{}{
\setlength{\parsep}{0em}
\setlength{\leftmargin}{5ex}
\setlength{\labelwidth}{2ex}
\setlength{\itemindent}{0ex}
\setlength{\topsep}{0pt}}
\item[\texttt{1-norm}] use the 1-norm
\item[\texttt{2-norm}] use the 2-norm
\item[\texttt{max-norm}] use the infinity norm
\end{list}
}

\printoption{corrector\_compl\_avrg\_red\_fact}%
{$0<\textrm{real}$}%
{$1$}%
{Complementarity tolerance factor for accepting corrector step (unsupported!).\\
This option determines the factor by which complementarity is allowed to increase for a corrector step to be accepted.}%
{}

\printoption{corrector\_type}%
{\ttfamily none, affine, primal-dual}%
{none}%
{The type of corrector steps that should be taken (unsupported!).\\
If "mu\_strategy" is "adaptive", this option determines what kind of corrector steps should be tried.}%
{\begin{list}{}{
\setlength{\parsep}{0em}
\setlength{\leftmargin}{5ex}
\setlength{\labelwidth}{2ex}
\setlength{\itemindent}{0ex}
\setlength{\topsep}{0pt}}
\item[\texttt{none}] no corrector
\item[\texttt{affine}] corrector step towards mu=0
\item[\texttt{primal-dual}] corrector step towards current mu
\end{list}
}

\printoption{delta}%
{$0<\textrm{real}$}%
{$1$}%
{Multiplier for constraint violation in the switching rule.\\
(See Eqn. (19) in the implementation paper.)}%
{}

\printoption{eta\_phi}%
{$0<\textrm{real}<0.5$}%
{$10^{- 8}$}%
{Relaxation factor in the Armijo condition.\\
(See Eqn. (20) in the implementation paper)}%
{}

\printoption{filter\_reset\_trigger}%
{$1\leq\textrm{integer}$}%
{$5$}%
{Number of iterations that trigger the filter reset.\\
If the filter reset heuristic is active and the number of successive iterations in which the last rejected trial step size was rejected because of the filter, the filter is reset.}%
{}

\printoption{gamma\_phi}%
{$0<\textrm{real}<1$}%
{$10^{- 8}$}%
{Relaxation factor in the filter margin for the barrier function.\\
(See Eqn. (18a) in the implementation paper.)}%
{}

\printoption{gamma\_theta}%
{$0<\textrm{real}<1$}%
{$10^{- 5}$}%
{Relaxation factor in the filter margin for the constraint violation.\\
(See Eqn. (18b) in the implementation paper.)}%
{}

\printoption{kappa\_sigma}%
{$0<\textrm{real}$}%
{$10^{ 10}$}%
{Factor limiting the deviation of dual variables from primal estimates.\\
If the dual variables deviate from their primal estimates, a correction is performed. (See Eqn. (16) in the implementation paper.) Setting the value to less than 1 disables the correction.}%
{}

\printoption{kappa\_soc}%
{$0<\textrm{real}$}%
{$0.99$}%
{Factor in the sufficient reduction rule for second order correction.\\
This option determines how much a second order correction step must reduce the constraint violation so that further correction steps are attempted.  (See Step A-5.9 of Algorithm A in the implementation paper.)}%
{}

\printoption{max\_filter\_resets}%
{$0\leq\textrm{integer}$}%
{$5$}%
{Maximal allowed number of filter resets\\
A positive number enables a heuristic that resets the filter, whenever in more than "filter\_reset\_trigger" successive iterations the last rejected trial steps size was rejected because of the filter.  This option determine the maximal number of resets that are allowed to take place.}%
{}

\printoption{max\_soc}%
{$0\leq\textrm{integer}$}%
{$4$}%
{Maximum number of second order correction trial steps at each iteration.\\
Choosing 0 disables the second order corrections. (This is p\^{max} of Step A-5.9 of Algorithm A in the implementation paper.)}%
{}

\printoption{nu\_inc}%
{$0<\textrm{real}$}%
{$0.0001$}%
{Increment of the penalty parameter.}%
{}

\printoption{nu\_init}%
{$0<\textrm{real}$}%
{$10^{- 6}$}%
{Initial value of the penalty parameter.}%
{}

\printoption{obj\_max\_inc}%
{$1<\textrm{real}$}%
{$5$}%
{Determines the upper bound on the acceptable increase of barrier objective function.\\
Trial points are rejected if they lead to an increase in the barrier objective function by more than obj\_max\_inc orders of magnitude.}%
{}

\printoption{recalc\_y}%
{\ttfamily no, yes}%
{no}%
{Tells the algorithm to recalculate the equality and inequality multipliers as least square estimates.\\
This asks the algorithm to recompute the multipliers, whenever the current infeasibility is less than recalc\_y\_feas\_tol. Choosing yes might be helpful in the quasi-Newton option.  However, each recalculation requires an extra factorization of the linear system.  If a limited memory quasi-Newton option is chosen, this is used by default.}%
{\begin{list}{}{
\setlength{\parsep}{0em}
\setlength{\leftmargin}{5ex}
\setlength{\labelwidth}{2ex}
\setlength{\itemindent}{0ex}
\setlength{\topsep}{0pt}}
\item[\texttt{no}] use the Newton step to update the multipliers
\item[\texttt{yes}] use least-square multiplier estimates
\end{list}
}

\printoption{recalc\_y\_feas\_tol}%
{$0<\textrm{real}$}%
{$10^{- 6}$}%
{Feasibility threshold for recomputation of multipliers.\\
If recalc\_y is chosen and the current infeasibility is less than this value, then the multipliers are recomputed.}%
{}

\printoption{rho}%
{$0<\textrm{real}<1$}%
{$0.1$}%
{Value in penalty parameter update formula.}%
{}

\printoption{s\_phi}%
{$1<\textrm{real}$}%
{$2.3$}%
{Exponent for linear barrier function model in the switching rule.\\
(See Eqn. (19) in the implementation paper.)}%
{}

\printoption{s\_theta}%
{$1<\textrm{real}$}%
{$1.1$}%
{Exponent for current constraint violation in the switching rule.\\
(See Eqn. (19) in the implementation paper.)}%
{}

\printoption{skip\_corr\_if\_neg\_curv}%
{\ttfamily no, yes}%
{yes}%
{Skip the corrector step in negative curvature iteration (unsupported!).\\
The corrector step is not tried if negative curvature has been encountered during the computation of the search direction in the current iteration. This option is only used if "mu\_strategy" is "adaptive".}%
{\begin{list}{}{
\setlength{\parsep}{0em}
\setlength{\leftmargin}{5ex}
\setlength{\labelwidth}{2ex}
\setlength{\itemindent}{0ex}
\setlength{\topsep}{0pt}}
\item[\texttt{no}] don't skip
\item[\texttt{yes}] skip
\end{list}
}

\printoption{skip\_corr\_in\_monotone\_mode}%
{\ttfamily no, yes}%
{yes}%
{Skip the corrector step during monotone barrier parameter mode (unsupported!).\\
The corrector step is not tried if the algorithm is currently in the monotone mode (see also option "barrier\_strategy").This option is only used if "mu\_strategy" is "adaptive".}%
{\begin{list}{}{
\setlength{\parsep}{0em}
\setlength{\leftmargin}{5ex}
\setlength{\labelwidth}{2ex}
\setlength{\itemindent}{0ex}
\setlength{\topsep}{0pt}}
\item[\texttt{no}] don't skip
\item[\texttt{yes}] skip
\end{list}
}

\printoption{slack\_move}%
{$0\leq\textrm{real}$}%
{$1.81899 \cdot 10^{-12}$}%
{Correction size for very small slacks.\\
Due to numerical issues or the lack of an interior, the slack variables might become very small.  If a slack becomes very small compared to machine precision, the corresponding bound is moved slightly.  This parameter determines how large the move should be.  Its default value is mach\_eps\^{3/4}.  (See also end of Section 3.5 in implementation paper - but actual implementation might be somewhat different.)}%
{}

\printoption{theta\_max\_fact}%
{$0<\textrm{real}$}%
{$10000$}%
{Determines upper bound for constraint violation in the filter.\\
The algorithmic parameter theta\_max is determined as theta\_max\_fact times the maximum of 1 and the constraint violation at initial point.  Any point with a constraint violation larger than theta\_max is unacceptable to the filter (see Eqn. (21) in the implementation paper).}%
{}

\printoption{theta\_min\_fact}%
{$0<\textrm{real}$}%
{$0.0001$}%
{Determines constraint violation threshold in the switching rule.\\
The algorithmic parameter theta\_min is determined as theta\_min\_fact times the maximum of 1 and the constraint violation at initial point.  The switching rules treats an iteration as an h-type iteration whenever the current constraint violation is larger than theta\_min (see paragraph before Eqn. (19) in the implementation paper).}%
{}

\printoption{tiny\_step\_tol}%
{$0\leq\textrm{real}$}%
{$2.22045 \cdot 10^{-15}$}%
{Tolerance for detecting numerically insignificant steps.\\
If the search direction in the primal variables (x and s) is, in relative terms for each component, less than this value, the algorithm accepts the full step without line search.  If this happens repeatedly, the algorithm will terminate with a corresponding exit message. The default value is 10 times machine precision.}%
{}

\printoption{tiny\_step\_y\_tol}%
{$0\leq\textrm{real}$}%
{$0.01$}%
{Tolerance for quitting because of numerically insignificant steps.\\
If the search direction in the primal variables (x and s) is, in relative terms for each component, repeatedly less than tiny\_step\_tol, and the step in the y variables is smaller than this threshold, the algorithm will terminate.}%
{}

\printoption{watchdog\_shortened\_iter\_trigger}%
{$0\leq\textrm{integer}$}%
{$10$}%
{Number of shortened iterations that trigger the watchdog.\\
If the number of successive iterations in which the backtracking line search did not accept the first trial point exceeds this number, the watchdog procedure is activated.  Choosing "0" here disables the watchdog procedure.}%
{}

\printoption{watchdog\_trial\_iter\_max}%
{$1\leq\textrm{integer}$}%
{$3$}%
{Maximum number of watchdog iterations.\\
This option determines the number of trial iterations allowed before the watchdog procedure is aborted and the algorithm returns to the stored point.}%
{}

\printoptioncategory{Linear Solver}
\printoption{hsl\_library}%
{string}%
{}%
{path and filename of HSL library for dynamic load\\
Specify the path to a library that contains HSL routines and can be load via dynamic linking. Note, that you still need to specify to use the corresponding routines (ma27, ...) by setting the corresponding options, e.g., ``linear\_solver''.}%
{}

\printoption{linear\_scaling\_on\_demand}%
{\ttfamily no, yes}%
{yes}%
{Flag indicating that linear scaling is only done if it seems required.\\
This option is only important if a linear scaling method (e.g., mc19) is used.  If you choose "no", then the scaling factors are computed for every linear system from the start.  This can be quite expensive. Choosing "yes" means that the algorithm will start the scaling method only when the solutions to the linear system seem not good, and then use it until the end.}%
{\begin{list}{}{
\setlength{\parsep}{0em}
\setlength{\leftmargin}{5ex}
\setlength{\labelwidth}{2ex}
\setlength{\itemindent}{0ex}
\setlength{\topsep}{0pt}}
\item[\texttt{no}] Always scale the linear system.
\item[\texttt{yes}] Start using linear system scaling if solutions seem not good.
\end{list}
}

\printoption{linear\_solver}%
{\ttfamily ma27, ma57, pardiso, mumps}%
{ma27}%
{Linear solver used for step computations.\\
Determines which linear algebra package is to be used for the solution of the augmented linear system (for obtaining the search directions). Note, that in order to use MA27, MA57, or Pardiso, a library with HSL or Pardiso code need to be provided (see Section 5.2 and options "hsl\_library" and "pardiso\_library").}%
{\begin{list}{}{
\setlength{\parsep}{0em}
\setlength{\leftmargin}{5ex}
\setlength{\labelwidth}{2ex}
\setlength{\itemindent}{0ex}
\setlength{\topsep}{0pt}}
\item[\texttt{ma27}] use the Harwell routine MA27
\item[\texttt{ma57}] use the Harwell routine MA57
\item[\texttt{pardiso}] use the Pardiso package
\item[\texttt{mumps}] use MUMPS package
\end{list}
}

\printoption{linear\_system\_scaling}%
{\ttfamily none, mc19, slack-based}%
{mc19}%
{Method for scaling the linear system.\\
Determines the method used to compute symmetric scaling factors for the augmented system (see also the "linear\_scaling\_on\_demand" option).  This scaling is independent of the NLP problem scaling.  By default, MC19 is only used if MA27 or MA57 are selected as linear solvers. This value is only available if Ipopt has been compiled with MC19.}%
{\begin{list}{}{
\setlength{\parsep}{0em}
\setlength{\leftmargin}{5ex}
\setlength{\labelwidth}{2ex}
\setlength{\itemindent}{0ex}
\setlength{\topsep}{0pt}}
\item[\texttt{none}] no scaling will be performed
\item[\texttt{mc19}] use the Harwell routine MC19
\item[\texttt{slack-based}] use the slack values
\end{list}
}

\printoption{pardiso\_library}%
{string}%
{}%
{path and filename of PARDISO library for dynamic load\\
Specify the path to a PARDISO library that and can be load via dynamic linking. Note, that you still need to specify to use pardiso as linear\_solver.}%
{}

\printoptioncategory{MA27 Linear Solver}
\printoption{ma27\_ignore\_singularity}%
{\ttfamily no, yes}%
{no}%
{Enables MA27's ability to solve a linear system even if the matrix is singular.\\
Setting this option to "yes" means that Ipopt will call MA27 to compute solutions for right hand sides, even if MA27 has detected that the matrix is singular (but is still able to solve the linear system). In some cases this might be better than using Ipopt's heuristic of small perturbation of the lower diagonal of the KKT matrix.}%
{\begin{list}{}{
\setlength{\parsep}{0em}
\setlength{\leftmargin}{5ex}
\setlength{\labelwidth}{2ex}
\setlength{\itemindent}{0ex}
\setlength{\topsep}{0pt}}
\item[\texttt{no}] Don't have MA27 solve singular systems
\item[\texttt{yes}] Have MA27 solve singular systems
\end{list}
}

\printoption{ma27\_la\_init\_factor}%
{$1\leq\textrm{real}$}%
{$5$}%
{Real workspace memory for MA27.\\
The initial real workspace memory = la\_init\_factor * memory required by unfactored system. Ipopt will increase the workspace size by meminc\_factor if required.  This option is only available if  Ipopt has been compiled with MA27.}%
{}

\printoption{ma27\_liw\_init\_factor}%
{$1\leq\textrm{real}$}%
{$5$}%
{Integer workspace memory for MA27.\\
The initial integer workspace memory = liw\_init\_factor * memory required by unfactored system. Ipopt will increase the workspace size by meminc\_factor if required.  This option is only available if Ipopt has been compiled with MA27.}%
{}

\printoption{ma27\_meminc\_factor}%
{$1\leq\textrm{real}$}%
{$10$}%
{Increment factor for workspace size for MA27.\\
If the integer or real workspace is not large enough, Ipopt will increase its size by this factor.  This option is only available if Ipopt has been compiled with MA27.}%
{}

\printoption{ma27\_pivtol}%
{$0<\textrm{real}<1$}%
{$10^{- 8}$}%
{Pivot tolerance for the linear solver MA27.\\
A smaller number pivots for sparsity, a larger number pivots for stability.  This option is only available if Ipopt has been compiled with MA27.}%
{}

\printoption{ma27\_pivtolmax}%
{$0<\textrm{real}<1$}%
{$0.0001$}%
{Maximum pivot tolerance for the linear solver MA27.\\
Ipopt may increase pivtol as high as pivtolmax to get a more accurate solution to the linear system.  This option is only available if Ipopt has been compiled with MA27.}%
{}

\printoption{ma27\_skip\_inertia\_check}%
{\ttfamily no, yes}%
{no}%
{Always pretend inertia is correct.\\
Setting this option to "yes" essentially disables inertia check. This option makes the algorithm non-robust and easily fail, but it might give some insight into the necessity of inertia control.}%
{\begin{list}{}{
\setlength{\parsep}{0em}
\setlength{\leftmargin}{5ex}
\setlength{\labelwidth}{2ex}
\setlength{\itemindent}{0ex}
\setlength{\topsep}{0pt}}
\item[\texttt{no}] check inertia
\item[\texttt{yes}] skip inertia check
\end{list}
}

\printoptioncategory{MA28 Linear Solver}
\printoption{ma28\_pivtol}%
{$0<\textrm{real}\leq1$}%
{$0.01$}%
{Pivot tolerance for linear solver MA28.\\
This is used when MA28 tries to find the dependent constraints.}%
{}

\printoptioncategory{MA57 Linear Solver}
\printoption{ma57\_automatic\_scaling}%
{\ttfamily no, yes}%
{yes}%
{Controls MA57 automatic scaling\\
This option controls the internal scaling option of MA57.This is ICNTL(15) in MA57.}%
{\begin{list}{}{
\setlength{\parsep}{0em}
\setlength{\leftmargin}{5ex}
\setlength{\labelwidth}{2ex}
\setlength{\itemindent}{0ex}
\setlength{\topsep}{0pt}}
\item[\texttt{no}] Do not scale the linear system matrix
\item[\texttt{yes}] Scale the linear system matrix
\end{list}
}

\printoption{ma57\_block\_size}%
{$1\leq\textrm{integer}$}%
{$16$}%
{Controls block size used by Level 3 BLAS in MA57BD\\
This is ICNTL(11) in MA57.}%
{}

\printoption{ma57\_node\_amalgamation}%
{$1\leq\textrm{integer}$}%
{$16$}%
{Node amalgamation parameter\\
This is ICNTL(12) in MA57.}%
{}

\printoption{ma57\_pivot\_order}%
{$0\leq\textrm{integer}\leq5$}%
{$5$}%
{Controls pivot order in MA57\\
This is ICNTL(6) in MA57.}%
{}

\printoption{ma57\_pivtol}%
{$0<\textrm{real}<1$}%
{$10^{- 8}$}%
{Pivot tolerance for the linear solver MA57.\\
A smaller number pivots for sparsity, a larger number pivots for stability. This option is only available if Ipopt has been compiled with MA57.}%
{}

\printoption{ma57\_pivtolmax}%
{$0<\textrm{real}<1$}%
{$0.0001$}%
{Maximum pivot tolerance for the linear solver MA57.\\
Ipopt may increase pivtol as high as ma57\_pivtolmax to get a more accurate solution to the linear system.  This option is only available if Ipopt has been compiled with MA57.}%
{}

\printoption{ma57\_pre\_alloc}%
{$1\leq\textrm{real}$}%
{$1.05$}%
{Safety factor for work space memory allocation for the linear solver MA57.\\
If 1 is chosen, the suggested amount of work space is used.  However, choosing a larger number might avoid reallocation if the suggest values do not suffice.  This option is only available if Ipopt has been compiled with MA57.}%
{}

\printoption{ma57\_small\_pivot\_flag}%
{$0\leq\textrm{integer}\leq1$}%
{$0$}%
{If set to 1, then when small entries defined by CNTL(2) are detected they are removed and the corresponding pivots placed at the end of the factorization.  This can be particularly efficient if the matrix is highly rank deficient.\\
This is ICNTL(16) in MA57.}%
{}

\printoptioncategory{Mumps Linear Solver}
\printoption{mumps\_dep\_tol}%
{$\textrm{real}$}%
{$-1$}%
{Pivot threshold for detection of linearly dependent constraints in MUMPS.\\
When MUMPS is used to determine linearly dependent constraints, this is determines the threshold for a pivot to be considered zero.  This is CNTL(3) in MUMPS.}%
{}

\printoption{mumps\_mem\_percent}%
{$0\leq\textrm{integer}$}%
{$1000$}%
{Percentage increase in the estimated working space for MUMPS.\\
In MUMPS when significant extra fill-in is caused by numerical pivoting, larger values of mumps\_mem\_percent may help use the workspace more efficiently.  On the other hand, if memory requirement are too large at the very beginning of the optimization, choosing a much smaller value for this option, such as 5, might reduce memory requirements.}%
{}

\printoption{mumps\_permuting\_scaling}%
{$0\leq\textrm{integer}\leq7$}%
{$7$}%
{Controls permuting and scaling in MUMPS\\
This is ICNTL(6) in MUMPS.}%
{}

\printoption{mumps\_pivot\_order}%
{$0\leq\textrm{integer}\leq7$}%
{$7$}%
{Controls pivot order in MUMPS\\
This is ICNTL(7) in MUMPS.}%
{}

\printoption{mumps\_pivtol}%
{$0\leq\textrm{real}\leq1$}%
{$10^{- 6}$}%
{Pivot tolerance for the linear solver MUMPS.\\
A smaller number pivots for sparsity, a larger number pivots for stability.  This option is only available if Ipopt has been compiled with MUMPS.}%
{}

\printoption{mumps\_pivtolmax}%
{$0\leq\textrm{real}\leq1$}%
{$0.1$}%
{Maximum pivot tolerance for the linear solver MUMPS.\\
Ipopt may increase pivtol as high as pivtolmax to get a more accurate solution to the linear system.  This option is only available if Ipopt has been compiled with MUMPS.}%
{}

\printoption{mumps\_scaling}%
{$-2\leq\textrm{integer}\leq77$}%
{$77$}%
{Controls scaling in MUMPS\\
This is ICNTL(8) in MUMPS.}%
{}

\printoptioncategory{NLP}
\printoption{bound\_relax\_factor}%
{$0\leq\textrm{real}$}%
{$10^{-10}$}%
{Factor for initial relaxation of the bounds.\\
Before start of the optimization, the bounds given by the user are relaxed.  This option sets the factor for this relaxation.  If it is set to zero, then then bounds relaxation is disabled. (See Eqn.(35) in implementation paper.)}%
{}

\printoption{check\_derivatives\_for\_naninf}%
{\ttfamily no, yes}%
{no}%
{Indicates whether it is desired to check for Nan/Inf in derivative matrices\\
Activating this option will cause an error if an invalid number is detected in the constraint Jacobians or the Lagrangian Hessian.  If this is not activated, the test is skipped, and the algorithm might proceed with invalid numbers and fail.  If test is activated and an invalid number is detected, the matrix is written to output with print\_level corresponding to J\_MORE\_DETAILED; so beware of large output!}%
{\begin{list}{}{
\setlength{\parsep}{0em}
\setlength{\leftmargin}{5ex}
\setlength{\labelwidth}{2ex}
\setlength{\itemindent}{0ex}
\setlength{\topsep}{0pt}}
\item[\texttt{no}] Don't check (faster).
\item[\texttt{yes}] Check Jacobians and Hessian for Nan and Inf.
\end{list}
}

\printoption{dependency\_detection\_with\_rhs}%
{\ttfamily no, yes}%
{no}%
{Indicates if the right hand sides of the constraints should be considered during dependency detection}%
{\begin{list}{}{
\setlength{\parsep}{0em}
\setlength{\leftmargin}{5ex}
\setlength{\labelwidth}{2ex}
\setlength{\itemindent}{0ex}
\setlength{\topsep}{0pt}}
\item[\texttt{no}] only look at gradients
\item[\texttt{yes}] also consider right hand side
\end{list}
}

\printoption{dependency\_detector}%
{\ttfamily none, mumps, wsmp, ma28}%
{none}%
{Indicates which linear solver should be used to detect linearly dependent equality constraints.\\
The default and available choices depend on how Ipopt has been compiled.  This is experimental and does not work well.}%
{\begin{list}{}{
\setlength{\parsep}{0em}
\setlength{\leftmargin}{5ex}
\setlength{\labelwidth}{2ex}
\setlength{\itemindent}{0ex}
\setlength{\topsep}{0pt}}
\item[\texttt{none}] don't check; no extra work at beginning
\item[\texttt{mumps}] use MUMPS
\item[\texttt{wsmp}] use WSMP
\item[\texttt{ma28}] use MA28
\end{list}
}

\printoption{fixed\_variable\_treatment}%
{\ttfamily make\_parameter, make\_constraint, relax\_bounds}%
{make\_parameter}%
{Determines how fixed variables should be handled.\\
The main difference between those options is that the starting point in the "make\_constraint" case still has the fixed variables at their given values, whereas in the case "make\_parameter" the functions are always evaluated with the fixed values for those variables.  Also, for "relax\_bounds", the fixing bound constraints are relaxed (according to" bound\_relax\_factor"). For both "make\_constraints" and "relax\_bounds", bound multipliers are computed for the fixed variables.}%
{\begin{list}{}{
\setlength{\parsep}{0em}
\setlength{\leftmargin}{5ex}
\setlength{\labelwidth}{2ex}
\setlength{\itemindent}{0ex}
\setlength{\topsep}{0pt}}
\item[\texttt{make\_parameter}] Remove fixed variable from optimization variables
\item[\texttt{make\_constraint}] Add equality constraints fixing variables
\item[\texttt{relax\_bounds}] Relax fixing bound constraints
\end{list}
}

\printoption{honor\_original\_bounds}%
{\ttfamily no, yes}%
{yes}%
{Indicates whether final points should be projected into original bounds.\\
Ipopt might relax the bounds during the optimization (see, e.g., option "bound\_relax\_factor").  This option determines whether the final point should be projected back into the user-provide original bounds after the optimization.}%
{\begin{list}{}{
\setlength{\parsep}{0em}
\setlength{\leftmargin}{5ex}
\setlength{\labelwidth}{2ex}
\setlength{\itemindent}{0ex}
\setlength{\topsep}{0pt}}
\item[\texttt{no}] Leave final point unchanged
\item[\texttt{yes}] Project final point back into original bounds
\end{list}
}

\printoption{jac\_c\_constant}%
{\ttfamily no, yes}%
{no}%
{Indicates whether all equality constraints are linear\\
Activating this option will cause Ipopt to ask for the Jacobian of the equality constraints only once from the NLP and reuse this information later.}%
{\begin{list}{}{
\setlength{\parsep}{0em}
\setlength{\leftmargin}{5ex}
\setlength{\labelwidth}{2ex}
\setlength{\itemindent}{0ex}
\setlength{\topsep}{0pt}}
\item[\texttt{no}] Don't assume that all equality constraints are linear
\item[\texttt{yes}] Assume that equality constraints Jacobian are constant
\end{list}
}

\printoption{jac\_d\_constant}%
{\ttfamily no, yes}%
{no}%
{Indicates whether all inequality constraints are linear\\
Activating this option will cause Ipopt to ask for the Jacobian of the inequality constraints only once from the NLP and reuse this information later.}%
{\begin{list}{}{
\setlength{\parsep}{0em}
\setlength{\leftmargin}{5ex}
\setlength{\labelwidth}{2ex}
\setlength{\itemindent}{0ex}
\setlength{\topsep}{0pt}}
\item[\texttt{no}] Don't assume that all inequality constraints are linear
\item[\texttt{yes}] Assume that equality constraints Jacobian are constant
\end{list}
}

\printoption{kappa\_d}%
{$0\leq\textrm{real}$}%
{$10^{- 5}$}%
{Weight for linear damping term (to handle one-sided bounds).\\
(see Section 3.7 in implementation paper.)}%
{}

\printoption{num\_linear\_variables}%
{$0\leq\textrm{integer}$}%
{$0$}%
{Number of linear variables\\
When the Hessian is approximated, it is assumed that the first num\_linear\_variables variables are linear.  The Hessian is then not approximated in this space.  If the get\_number\_of\_nonlinear\_variables method in the TNLP is implemented, this option is ignored.}%
{}

\printoptioncategory{NLP Scaling}
\printoption{nlp\_scaling\_constr\_target\_gradient}%
{$0\leq\textrm{real}$}%
{$0$}%
{Target value for constraint function gradient size.\\
If a positive number is chosen, the scaling factor the constraint functions is computed so that the gradient has the max norm of the given size at the starting point.  This overrides nlp\_scaling\_max\_gradient for the constraint functions.}%
{}

\printoption{nlp\_scaling\_max\_gradient}%
{$0<\textrm{real}$}%
{$100$}%
{Maximum gradient after NLP scaling.\\
This is the gradient scaling cut-off. If the maximum gradient is above this value, then gradient based scaling will be performed. Scaling parameters are calculated to scale the maximum gradient back to this value. (This is g\_max in Section 3.8 of the implementation paper.) Note: This option is only used if "nlp\_scaling\_method" is chosen as "gradient-based".}%
{}

\printoption{nlp\_scaling\_method}%
{\ttfamily none, user-scaling, gradient-based, equilibration-based}%
{gradient-based}%
{Select the technique used for scaling the NLP.\\
Selects the technique used for scaling the problem internally before it is solved. For user-scaling, the parameters come from the NLP. If you are using AMPL, they can be specified through suffixes ("scaling\_factor")}%
{\begin{list}{}{
\setlength{\parsep}{0em}
\setlength{\leftmargin}{5ex}
\setlength{\labelwidth}{2ex}
\setlength{\itemindent}{0ex}
\setlength{\topsep}{0pt}}
\item[\texttt{none}] no problem scaling will be performed
\item[\texttt{user-scaling}] scaling parameters will come from the user
\item[\texttt{gradient-based}] scale the problem so the maximum gradient at the starting point is scaling\_max\_gradient
\item[\texttt{equilibration-based}] scale the problem so that first derivatives are of order 1 at random points (only available with MC19)
\end{list}
}

\printoption{nlp\_scaling\_min\_value}%
{$0\leq\textrm{real}$}%
{$10^{- 8}$}%
{Minimum value of gradient-based scaling values.\\
This is the lower bound for the scaling factors computed by gradient-based scaling method.  If some derivatives of some functions are huge, the scaling factors will otherwise become very small, and the (unscaled) final constraint violation, for example, might then be significant.  Note: This option is only used if "nlp\_scaling\_method" is chosen as "gradient-based".}%
{}

\printoption{nlp\_scaling\_obj\_target\_gradient}%
{$0\leq\textrm{real}$}%
{$0$}%
{Target value for objective function gradient size.\\
If a positive number is chosen, the scaling factor the objective function is computed so that the gradient has the max norm of the given size at the starting point.  This overrides nlp\_scaling\_max\_gradient for the objective function.}%
{}

\printoptioncategory{Output}
\printoption{print\_info\_string}%
{\ttfamily no, yes}%
{no}%
{Enables printing of additional info string at end of iteration output.\\
This string contains some insider information about the current iteration.}%
{\begin{list}{}{
\setlength{\parsep}{0em}
\setlength{\leftmargin}{5ex}
\setlength{\labelwidth}{2ex}
\setlength{\itemindent}{0ex}
\setlength{\topsep}{0pt}}
\item[\texttt{no}] don't print string
\item[\texttt{yes}] print string at end of each iteration output
\end{list}
}

\printoption{print\_level}%
{$0\leq\textrm{integer}\leq12$}%
{$5$}%
{Output verbosity level.\\
Sets the default verbosity level for console output. The larger this value the more detailed is the output.}%
{}

\printoption{print\_timing\_statistics}%
{\ttfamily no, yes}%
{no}%
{Switch to print timing statistics.\\
If selected, the program will print the CPU usage (user time) for selected tasks.}%
{\begin{list}{}{
\setlength{\parsep}{0em}
\setlength{\leftmargin}{5ex}
\setlength{\labelwidth}{2ex}
\setlength{\itemindent}{0ex}
\setlength{\topsep}{0pt}}
\item[\texttt{no}] don't print statistics
\item[\texttt{yes}] print all timing statistics
\end{list}
}

\printoption{replace\_bounds}%
{\ttfamily no, yes}%
{no}%
{Indicates if all variable bounds should be replaced by inequality constraints\\
This option must be set for the inexact algorithm}%
{\begin{list}{}{
\setlength{\parsep}{0em}
\setlength{\leftmargin}{5ex}
\setlength{\labelwidth}{2ex}
\setlength{\itemindent}{0ex}
\setlength{\topsep}{0pt}}
\item[\texttt{no}] leave bounds on variables
\item[\texttt{yes}] replace variable bounds by inequality constraints
\end{list}
}

\printoptioncategory{Pardiso Linear Solver}
\printoption{pardiso\_iter\_coarse\_size}%
{$1\leq\textrm{integer}$}%
{$5000$}%
{Maximum Size of Coarse Grid Matrix\\
DPARM(3)}%
{}

\printoption{pardiso\_iter\_dropping\_factor}%
{$0<\textrm{real}<1$}%
{$0.5$}%
{dropping value for incomplete factor\\
DPARM(5)}%
{}

\printoption{pardiso\_iter\_dropping\_schur}%
{$0<\textrm{real}<1$}%
{$0.1$}%
{dropping value for sparsify schur complement factor\\
DPARM(6)}%
{}

\printoption{pardiso\_iter\_inverse\_norm\_factor}%
{$1<\textrm{real}$}%
{$5 \cdot 10^{  6}$}%
{\\
DPARM(8)}%
{}

\printoption{pardiso\_iter\_max\_levels}%
{$1\leq\textrm{integer}$}%
{$10000$}%
{Maximum Size of Grid Levels\\
DPARM(4)}%
{}

\printoption{pardiso\_iter\_max\_row\_fill}%
{$1\leq\textrm{integer}$}%
{$10000000$}%
{max fill for each row\\
DPARM(7)}%
{}

\printoption{pardiso\_iter\_relative\_tol}%
{$0<\textrm{real}<1$}%
{$10^{- 6}$}%
{Relative Residual Convergence\\
DPARM(2)}%
{}

\printoption{pardiso\_iterative}%
{\ttfamily no, yes}%
{no}%
{Switch on iterative solver in Pardiso library}%
{}

\printoption{pardiso\_matching\_strategy}%
{\ttfamily complete, complete+2x2, constraints}%
{complete+2x2}%
{Matching strategy to be used by Pardiso\\
This is IPAR(13) in Pardiso manual.  This option is only available if Ipopt has been compiled with Pardiso.}%
{\begin{list}{}{
\setlength{\parsep}{0em}
\setlength{\leftmargin}{5ex}
\setlength{\labelwidth}{2ex}
\setlength{\itemindent}{0ex}
\setlength{\topsep}{0pt}}
\item[\texttt{complete}] Match complete (IPAR(13)=1)
\item[\texttt{complete+2x2}] Match complete+2x2 (IPAR(13)=2)
\item[\texttt{constraints}] Match constraints (IPAR(13)=3)
\end{list}
}

\printoption{pardiso\_max\_droptol\_corrections}%
{$1\leq\textrm{integer}$}%
{$4$}%
{Maximal number of decreases of drop tolerance during one solve.\\
This is relevant only for iterative Pardiso options.}%
{}

\printoption{pardiso\_max\_iter}%
{$1\leq\textrm{integer}$}%
{$500$}%
{Maximum number of Krylov-Subspace Iteration\\
DPARM(1)}%
{}

\printoption{pardiso\_msglvl}%
{$0\leq\textrm{integer}$}%
{$0$}%
{Pardiso message level\\
This determines the amount of analysis output from the Pardiso solver. This is MSGLVL in the Pardiso manual.}%
{}

\printoption{pardiso\_out\_of\_core\_power}%
{$0\leq\textrm{integer}$}%
{$0$}%
{Enables out-of-core variant of Pardiso\\
Setting this option to a positive integer k makes Pardiso work in the out-of-core variant where the factor is split in 2\^k subdomains.  This is IPARM(50) in the Pardiso manual.  This option is only available if Ipopt has been compiled with Pardiso.}%
{}

\printoption{pardiso\_redo\_symbolic\_fact\_only\_if\_inertia\_wrong}%
{\ttfamily no, yes}%
{no}%
{Toggle for handling case when elements were perturbed by Pardiso.\\
This option is only available if Ipopt has been compiled with Pardiso.}%
{\begin{list}{}{
\setlength{\parsep}{0em}
\setlength{\leftmargin}{5ex}
\setlength{\labelwidth}{2ex}
\setlength{\itemindent}{0ex}
\setlength{\topsep}{0pt}}
\item[\texttt{no}] Always redo symbolic factorization when elements were perturbed
\item[\texttt{yes}] Only redo symbolic factorization when elements were perturbed if also the inertia was wrong
\end{list}
}

\printoption{pardiso\_repeated\_perturbation\_means\_singular}%
{\ttfamily no, yes}%
{no}%
{Interpretation of perturbed elements.\\
This option is only available if Ipopt has been compiled with Pardiso.}%
{\begin{list}{}{
\setlength{\parsep}{0em}
\setlength{\leftmargin}{5ex}
\setlength{\labelwidth}{2ex}
\setlength{\itemindent}{0ex}
\setlength{\topsep}{0pt}}
\item[\texttt{no}] Don't assume that matrix is singular if elements were perturbed after recent symbolic factorization
\item[\texttt{yes}] Assume that matrix is singular if elements were perturbed after recent symbolic factorization
\end{list}
}

\printoption{pardiso\_skip\_inertia\_check}%
{\ttfamily no, yes}%
{no}%
{Always pretent inertia is correct.\\
Setting this option to "yes" essentially disables inertia check. This option makes the algorithm non-robust and easily fail, but it might give some insight into the necessity of inertia control.}%
{\begin{list}{}{
\setlength{\parsep}{0em}
\setlength{\leftmargin}{5ex}
\setlength{\labelwidth}{2ex}
\setlength{\itemindent}{0ex}
\setlength{\topsep}{0pt}}
\item[\texttt{no}] check inertia
\item[\texttt{yes}] skip inertia check
\end{list}
}

\printoptioncategory{Restoration Phase}
\printoption{bound\_mult\_reset\_threshold}%
{$0\leq\textrm{real}$}%
{$1000$}%
{Threshold for resetting bound multipliers after the restoration phase.\\
After returning from the restoration phase, the bound multipliers are updated with a Newton step for complementarity.  Here, the change in the primal variables during the entire restoration phase is taken to be the corresponding primal Newton step. However, if after the update the largest bound multiplier exceeds the threshold specified by this option, the multipliers are all reset to 1.}%
{}

\printoption{constr\_mult\_reset\_threshold}%
{$0\leq\textrm{real}$}%
{$0$}%
{Threshold for resetting equality and inequality multipliers after restoration phase.\\
After returning from the restoration phase, the constraint multipliers are recomputed by a least square estimate.  This option triggers when those least-square estimates should be ignored.}%
{}

\printoption{evaluate\_orig\_obj\_at\_resto\_trial}%
{\ttfamily no, yes}%
{yes}%
{Determines if the original objective function should be evaluated at restoration phase trial points.\\
Setting this option to "yes" makes the restoration phase algorithm evaluate the objective function of the original problem at every trial point encountered during the restoration phase, even if this value is not required.  In this way, it is guaranteed that the original objective function can be evaluated without error at all accepted iterates; otherwise the algorithm might fail at a point where the restoration phase accepts an iterate that is good for the restoration phase problem, but not the original problem.  On the other hand, if the evaluation of the original objective is expensive, this might be costly.}%
{\begin{list}{}{
\setlength{\parsep}{0em}
\setlength{\leftmargin}{5ex}
\setlength{\labelwidth}{2ex}
\setlength{\itemindent}{0ex}
\setlength{\topsep}{0pt}}
\item[\texttt{no}] skip evaluation
\item[\texttt{yes}] evaluate at every trial point
\end{list}
}

\printoption{expect\_infeasible\_problem}%
{\ttfamily no, yes}%
{no}%
{Enable heuristics to quickly detect an infeasible problem.\\
This options is meant to activate heuristics that may speed up the infeasibility determination if you expect that there is a good chance for the problem to be infeasible.  In the filter line search procedure, the restoration phase is called more quickly than usually, and more reduction in the constraint violation is enforced before the restoration phase is left. If the problem is square, this option is enabled automatically.}%
{\begin{list}{}{
\setlength{\parsep}{0em}
\setlength{\leftmargin}{5ex}
\setlength{\labelwidth}{2ex}
\setlength{\itemindent}{0ex}
\setlength{\topsep}{0pt}}
\item[\texttt{no}] the problem probably be feasible
\item[\texttt{yes}] the problem has a good chance to be infeasible
\end{list}
}

\printoption{expect\_infeasible\_problem\_ctol}%
{$0\leq\textrm{real}$}%
{$0.001$}%
{Threshold for disabling "expect\_infeasible\_problem" option.\\
If the constraint violation becomes smaller than this threshold, the "expect\_infeasible\_problem" heuristics in the filter line search are disabled. If the problem is square, this options is set to 0.}%
{}

\printoption{expect\_infeasible\_problem\_ytol}%
{$0<\textrm{real}$}%
{$10^{  8}$}%
{Multiplier threshold for activating "expect\_infeasible\_problem" option.\\
If the max norm of the constraint multipliers becomes larger than this value and "expect\_infeasible\_problem" is chosen, then the restoration phase is entered.}%
{}

\printoption{max\_resto\_iter}%
{$0\leq\textrm{integer}$}%
{$3000000$}%
{Maximum number of successive iterations in restoration phase.\\
The algorithm terminates with an error message if the number of iterations successively taken in the restoration phase exceeds this number.}%
{}

\printoption{max\_soft\_resto\_iters}%
{$0\leq\textrm{integer}$}%
{$10$}%
{Maximum number of iterations performed successively in soft restoration phase.\\
If the soft restoration phase is performed for more than so many iterations in a row, the regular restoration phase is called.}%
{}

\printoption{required\_infeasibility\_reduction}%
{$0\leq\textrm{real}<1$}%
{$0.9$}%
{Required reduction of infeasibility before leaving restoration phase.\\
The restoration phase algorithm is performed, until a point is found that is acceptable to the filter and the infeasibility has been reduced by at least the fraction given by this option.}%
{}

\printoption{resto\_penalty\_parameter}%
{$0<\textrm{real}$}%
{$1000$}%
{Penalty parameter in the restoration phase objective function.\\
This is the parameter rho in equation (31a) in the Ipopt implementation paper.}%
{}

\printoption{soft\_resto\_pderror\_reduction\_factor}%
{$0\leq\textrm{real}$}%
{$0.9999$}%
{Required reduction in primal-dual error in the soft restoration phase.\\
The soft restoration phase attempts to reduce the primal-dual error with regular steps. If the damped primal-dual step (damped only to satisfy the fraction-to-the-boundary rule) is not decreasing the primal-dual error by at least this factor, then the regular restoration phase is called. Choosing "0" here disables the soft restoration phase.}%
{}

\printoption{start\_with\_resto}%
{\ttfamily no, yes}%
{no}%
{Tells algorithm to switch to restoration phase in first iteration.\\
Setting this option to "yes" forces the algorithm to switch to the feasibility restoration phase in the first iteration. If the initial point is feasible, the algorithm will abort with a failure.}%
{\begin{list}{}{
\setlength{\parsep}{0em}
\setlength{\leftmargin}{5ex}
\setlength{\labelwidth}{2ex}
\setlength{\itemindent}{0ex}
\setlength{\topsep}{0pt}}
\item[\texttt{no}] don't force start in restoration phase
\item[\texttt{yes}] force start in restoration phase
\end{list}
}

\printoptioncategory{Step Calculation}
\printoption{fast\_step\_computation}%
{\ttfamily no, yes}%
{no}%
{Indicates if the linear system should be solved quickly.\\
If set to yes, the algorithm assumes that the linear system that is solved to obtain the search direction, is solved sufficiently well. In that case, no residuals are computed, and the computation of the search direction is a little faster.}%
{\begin{list}{}{
\setlength{\parsep}{0em}
\setlength{\leftmargin}{5ex}
\setlength{\labelwidth}{2ex}
\setlength{\itemindent}{0ex}
\setlength{\topsep}{0pt}}
\item[\texttt{no}] Verify solution of linear system by computing residuals.
\item[\texttt{yes}] Trust that linear systems are solved well.
\end{list}
}

\printoption{first\_hessian\_perturbation}%
{$0<\textrm{real}$}%
{$0.0001$}%
{Size of first x-s perturbation tried.\\
The first value tried for the x-s perturbation in the inertia correction scheme.(This is delta\_0 in the implementation paper.)}%
{}

\printoption{jacobian\_regularization\_exponent}%
{$0\leq\textrm{real}$}%
{$0.25$}%
{Exponent for mu in the regularization for rank-deficient constraint Jacobians.\\
(This is kappa\_c in the implementation paper.)}%
{}

\printoption{jacobian\_regularization\_value}%
{$0\leq\textrm{real}$}%
{$10^{- 8}$}%
{Size of the regularization for rank-deficient constraint Jacobians.\\
(This is bar delta\_c in the implementation paper.)}%
{}

\printoption{max\_hessian\_perturbation}%
{$0<\textrm{real}$}%
{$10^{ 20}$}%
{Maximum value of regularization parameter for handling negative curvature.\\
In order to guarantee that the search directions are indeed proper descent directions, Ipopt requires that the inertia of the (augmented) linear system for the step computation has the correct number of negative and positive eigenvalues. The idea is that this guides the algorithm away from maximizers and makes Ipopt more likely converge to first order optimal points that are minimizers. If the inertia is not correct, a multiple of the identity matrix is added to the Hessian of the Lagrangian in the augmented system. This parameter gives the maximum value of the regularization parameter. If a regularization of that size is not enough, the algorithm skips this iteration and goes to the restoration phase. (This is delta\_w\^max in the implementation paper.)}%
{}

\printoption{max\_refinement\_steps}%
{$0\leq\textrm{integer}$}%
{$10$}%
{Maximum number of iterative refinement steps per linear system solve.\\
Iterative refinement (on the full unsymmetric system) is performed for each right hand side.  This option determines the maximum number of iterative refinement steps.}%
{}

\printoption{mehrotra\_algorithm}%
{\ttfamily no, yes}%
{no}%
{Indicates if we want to do Mehrotra's algorithm.\\
If set to yes, Ipopt runs as Mehrotra's predictor-corrector algorithm. This works usually very well for LPs and convex QPs.  This automatically disables the line search, and chooses the (unglobalized) adaptive mu strategy with the "probing" oracle, and uses "corrector\_type=affine" without any safeguards; you should not set any of those options explicitly in addition.  Also, unless otherwise specified, the values of "bound\_push", "bound\_frac", and "bound\_mult\_init\_val" are set more aggressive, and sets "alpha\_for\_y=bound\_mult".}%
{\begin{list}{}{
\setlength{\parsep}{0em}
\setlength{\leftmargin}{5ex}
\setlength{\labelwidth}{2ex}
\setlength{\itemindent}{0ex}
\setlength{\topsep}{0pt}}
\item[\texttt{no}] Do the usual Ipopt algorithm.
\item[\texttt{yes}] Do Mehrotra's predictor-corrector algorithm.
\end{list}
}

\printoption{min\_hessian\_perturbation}%
{$0\leq\textrm{real}$}%
{$10^{-20}$}%
{Smallest perturbation of the Hessian block.\\
The size of the perturbation of the Hessian block is never selected smaller than this value, unless no perturbation is necessary. (This is delta\_w\^min in implementation paper.)}%
{}

\printoption{min\_refinement\_steps}%
{$0\leq\textrm{integer}$}%
{$1$}%
{Minimum number of iterative refinement steps per linear system solve.\\
Iterative refinement (on the full unsymmetric system) is performed for each right hand side.  This option determines the minimum number of iterative refinements (i.e. at least "min\_refinement\_steps" iterative refinement steps are enforced per right hand side.)}%
{}

\printoption{neg\_curv\_test\_tol}%
{$0<\textrm{real}$}%
{$0$}%
{Tolerance for heuristic to ignore wrong inertia.\\
If positive, incorrect inertia in the augmented system is ignored, and we test if the direction is a direction of positive curvature.  This tolerance determines when the direction is considered to be sufficiently positive.}%
{}

\printoption{perturb\_always\_cd}%
{\ttfamily no, yes}%
{no}%
{Active permanent perturbation of constraint linearization.\\
This options makes the delta\_c and delta\_d perturbation be used for the computation of every search direction.  Usually, it is only used when the iteration matrix is singular.}%
{\begin{list}{}{
\setlength{\parsep}{0em}
\setlength{\leftmargin}{5ex}
\setlength{\labelwidth}{2ex}
\setlength{\itemindent}{0ex}
\setlength{\topsep}{0pt}}
\item[\texttt{no}] perturbation only used when required
\item[\texttt{yes}] always use perturbation
\end{list}
}

\printoption{perturb\_dec\_fact}%
{$0<\textrm{real}<1$}%
{$0.333333$}%
{Decrease factor for x-s perturbation.\\
The factor by which the perturbation is decreased when a trial value is deduced from the size of the most recent successful perturbation. (This is kappa\_w\^- in the implementation paper.)}%
{}

\printoption{perturb\_inc\_fact}%
{$1<\textrm{real}$}%
{$8$}%
{Increase factor for x-s perturbation.\\
The factor by which the perturbation is increased when a trial value was not sufficient - this value is used for the computation of all perturbations except for the first. (This is kappa\_w\^+ in the implementation paper.)}%
{}

\printoption{perturb\_inc\_fact\_first}%
{$1<\textrm{real}$}%
{$100$}%
{Increase factor for x-s perturbation for very first perturbation.\\
The factor by which the perturbation is increased when a trial value was not sufficient - this value is used for the computation of the very first perturbation and allows a different value for for the first perturbation than that used for the remaining perturbations. (This is bar\_kappa\_w\^+ in the implementation paper.)}%
{}

\printoption{residual\_improvement\_factor}%
{$0<\textrm{real}$}%
{$1$}%
{Minimal required reduction of residual test ratio in iterative refinement.\\
If the improvement of the residual test ratio made by one iterative refinement step is not better than this factor, iterative refinement is aborted.}%
{}

\printoption{residual\_ratio\_max}%
{$0<\textrm{real}$}%
{$10^{-10}$}%
{Iterative refinement tolerance\\
Iterative refinement is performed until the residual test ratio is less than this tolerance (or until "max\_refinement\_steps" refinement steps are performed).}%
{}

\printoption{residual\_ratio\_singular}%
{$0<\textrm{real}$}%
{$10^{- 5}$}%
{Threshold for declaring linear system singular after failed iterative refinement.\\
If the residual test ratio is larger than this value after failed iterative refinement, the algorithm pretends that the linear system is singular.}%
{}

\printoptioncategory{Warm Start}
\printoption{warm\_start\_bound\_frac}%
{$0<\textrm{real}\leq0.5$}%
{$0.001$}%
{same as bound\_frac for the regular initializer.}%
{}

\printoption{warm\_start\_bound\_push}%
{$0<\textrm{real}$}%
{$0.001$}%
{same as bound\_push for the regular initializer.}%
{}

\printoption{warm\_start\_entire\_iterate}%
{\ttfamily no, yes}%
{no}%
{Tells algorithm whether to use the GetWarmStartIterate method in the NLP.}%
{\begin{list}{}{
\setlength{\parsep}{0em}
\setlength{\leftmargin}{5ex}
\setlength{\labelwidth}{2ex}
\setlength{\itemindent}{0ex}
\setlength{\topsep}{0pt}}
\item[\texttt{no}] call GetStartingPoint in the NLP
\item[\texttt{yes}] call GetWarmStartIterate in the NLP
\end{list}
}

\printoption{warm\_start\_init\_point}%
{\ttfamily no, yes}%
{no}%
{Warm-start for initial point\\
Indicates whether this optimization should use a warm start initialization, where values of primal and dual variables are given (e.g., from a previous optimization of a related problem.)}%
{\begin{list}{}{
\setlength{\parsep}{0em}
\setlength{\leftmargin}{5ex}
\setlength{\labelwidth}{2ex}
\setlength{\itemindent}{0ex}
\setlength{\topsep}{0pt}}
\item[\texttt{no}] do not use the warm start initialization
\item[\texttt{yes}] use the warm start initialization
\end{list}
}

\printoption{warm\_start\_mult\_bound\_push}%
{$0<\textrm{real}$}%
{$0.001$}%
{same as mult\_bound\_push for the regular initializer.}%
{}

\printoption{warm\_start\_mult\_init\_max}%
{$\textrm{real}$}%
{$10^{  6}$}%
{Maximum initial value for the equality multipliers.}%
{}

\printoption{warm\_start\_same\_structure}%
{\ttfamily no, yes}%
{no}%
{Indicates whether a problem with a structure identical to the previous one is to be solved.\\
If "yes" is chosen, then the algorithm assumes that an NLP is now to be solved, whose structure is identical to one that already was considered (with the same NLP object).}%
{\begin{list}{}{
\setlength{\parsep}{0em}
\setlength{\leftmargin}{5ex}
\setlength{\labelwidth}{2ex}
\setlength{\itemindent}{0ex}
\setlength{\topsep}{0pt}}
\item[\texttt{no}] Assume this is a new problem.
\item[\texttt{yes}] Assume this is problem has known structure
\end{list}
}

\printoption{warm\_start\_slack\_bound\_frac}%
{$0<\textrm{real}\leq0.5$}%
{$0.001$}%
{same as slack\_bound\_frac for the regular initializer.}%
{}

\printoption{warm\_start\_slack\_bound\_push}%
{$0<\textrm{real}$}%
{$0.001$}%
{same as slack\_bound\_push for the regular initializer.}%
{}

